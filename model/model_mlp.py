import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import os
import sys
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)
# å¼•ç”¨å¼•æ“
try:
    from alpha_engine import AlphaContext
except ImportError:
    print("é”™è¯¯: æœªæ‰¾åˆ° alpha_engine.py")
    sys.exit(1)

# ================= é…ç½® =================
DATA_PATH = "./data/market_data.csv"
REPORT_FILE = "./reports/all_reports.json" # ä»æ‰¹é‡è®¡ç®—çš„ç»“æœä¸­è¯»å–å› å­åˆ—è¡¨
INITIAL_CAPITAL = 1000000
TOP_K = 5

# å¦‚æœæ²¡æœ‰æŠ¥å‘Šæ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨å‡ ä¸ªç»å…¸å› å­åšæ¼”ç¤º
DEFAULT_ALPHAS = [
    {"name": "Momentum_10", "formula": "CLOSE / DELAY(CLOSE, 10) - 1"},
    {"name": "Reversion_5", "formula": "MA(CLOSE, 5) - CLOSE"},
    {"name": "Vol_20", "formula": "-1 * STD(CLOSE, 20)"},
    {"name": "Volume_Shock", "formula": "VOLUME / MA(VOLUME, 20)"}
]

def prepare_dataset(df, alpha_list):
    """
    è®¡ç®—æ‰€æœ‰å› å­çš„å€¼ï¼Œå¹¶å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªç‰¹å¾çŸ©é˜µ X
    """
    print("æ­£åœ¨æ„å»ºç¥ç»ç½‘ç»œè®­ç»ƒæ•°æ®...")
    
    # 1. è®¡ç®—ç›®æ ‡ (ä¸‹æœŸæ”¶ç›Šç‡)
    df = df.sort_values(['code', 'date']).reset_index(drop=True)
    df['target'] = df.groupby('code')['close'].shift(-1) / df['close'] - 1
    
    # 2. è®¡ç®—ç‰¹å¾ (æ‰€æœ‰å› å­å€¼)
    ctx = AlphaContext(df)
    env = {
        'CLOSE': ctx.CLOSE(), 'OPEN': ctx.OPEN(), 'VOLUME': ctx.VOLUME(), 
        'HIGH': ctx.HIGH(), 'LOW': ctx.LOW(),
        'DELAY': ctx.DELAY, 'MA': ctx.MA, 'STD': ctx.STD,
        'TS_MAX': ctx.TS_MAX, 'TS_MIN': ctx.TS_MIN,
        'CORR': ctx.CORR, 'RANK': ctx.RANK
    }
    
    feature_cols = []
    
    for alpha in alpha_list:
        name = alpha['name']
        formula = alpha['formula']
        col_name = f"feat_{name}"
        print(f"   - è®¡ç®—ç‰¹å¾: {name} ...", end="")
        try:
            df[col_name] = eval(formula, {}, env)
            feature_cols.append(col_name)
            print(" [å®Œæˆ]")
        except Exception as e:
            print(f" [å¤±è´¥] {e}")
            
    # 3. æ¸…æ´—æ•°æ® (å»é™¤ NaN)
    # æˆ‘ä»¬éœ€è¦ç‰¹å¾å’Œç›®æ ‡éƒ½ä¸ä¸ºç©º
    clean_df = df.dropna(subset=feature_cols + ['target']).copy()
    
    # æŒ‰ç…§æ—¶é—´æ’åºï¼Œè¿™ç‚¹å¯¹é‡‘èæ•°æ®å¾ˆé‡è¦
    clean_df = clean_df.sort_values('date')
    
    return clean_df, feature_cols

def train_dnn_strategy():
    # 1. åŠ è½½å› å­åˆ—è¡¨
    if os.path.exists(REPORT_FILE):
        with open(REPORT_FILE, 'r', encoding='utf-8') as f:
            alpha_config = json.load(f)
            # è¿‡æ»¤æ‰ä¹‹å‰çš„æŠ¥é”™å› å­ï¼Œåªå–æˆåŠŸçš„
            alpha_config = [a for a in alpha_config if 'error' not in a]
            # ä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œå¦‚æœå› å­å¤ªå¤šï¼Œåªå–å‰10ä¸ª
            alpha_config = alpha_config[:10]
    else:
        print("æœªæ‰¾åˆ°å› å­æŠ¥å‘Šï¼Œä½¿ç”¨é»˜è®¤å› å­åˆ—è¡¨ã€‚")
        alpha_config = DEFAULT_ALPHAS

    print(f"å³å°†ç»„åˆ {len(alpha_config)} ä¸ª Alpha å› å­è¿›è¡Œè®­ç»ƒã€‚")

    # 2. åŠ è½½æ•°æ®
    if not os.path.exists(DATA_PATH):
        print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
        return
    df = pd.read_csv(DATA_PATH)
    df['date'] = pd.to_datetime(df['date'])
    
    # 3. å‡†å¤‡ç‰¹å¾çŸ©é˜µ
    full_df, feature_cols = prepare_dataset(df, alpha_config)
    
    if full_df.empty:
        print("é”™è¯¯: è®­ç»ƒæ•°æ®ä¸ºç©ºã€‚")
        return

    # 4. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (æŒ‰æ—¶é—´åˆ‡åˆ†ï¼Œæ¨¡æ‹ŸçœŸå®å›æµ‹)
    # å‰ 80% æ—¶é—´ç”¨äºè®­ç»ƒ MLPï¼Œå 20% æ—¶é—´ç”¨äºå›æµ‹
    dates = np.sort(full_df['date'].unique())
    split_idx = int(len(dates) * 0.8)
    split_date = dates[split_idx]
    
    train_df = full_df[full_df['date'] < split_date]
    test_df = full_df[full_df['date'] >= split_date]
    
    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {train_df['date'].min().date()} -> {train_df['date'].max().date()} ({len(train_df)} æ ·æœ¬)")
    print(f"   æµ‹è¯•é›†: {test_df['date'].min().date()} -> {test_df['date'].max().date()} ({len(test_df)} æ ·æœ¬)")
    
    X_train = train_df[feature_cols].values
    y_train = train_df['target'].values
    X_test = test_df[feature_cols].values
    
    # 5. æ•°æ®æ ‡å‡†åŒ– (ç¥ç»ç½‘ç»œå¯¹æ•°å€¼èŒƒå›´æ•æ„Ÿ)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 6. å®šä¹‰å¹¶è®­ç»ƒ MLP (å¯¹åº”è®ºæ–‡ 3.4 èŠ‚)
    # "Hidden layer with ten ReLU-activated nodes"
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ MLP ç¥ç»ç½‘ç»œ (Weight Optimization)...")
    mlp = MLPRegressor(
        hidden_layer_sizes=(10,), # è®ºæ–‡è®¾å®šï¼š1ä¸ªéšè—å±‚ï¼Œ10ä¸ªèŠ‚ç‚¹
        activation='relu',
        solver='adam',
        max_iter=500,
        random_state=42,
        alpha=0.001 # L2 æ­£åˆ™åŒ–
    )
    
    mlp.fit(X_train_scaled, y_train)
    print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! å½“å‰ Loss: {mlp.loss_:.6f}")
    
    # 7. åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹ä¿¡å· (Composite Alpha)
    print("\næ­£åœ¨ç”Ÿæˆæµ‹è¯•é›†äº¤æ˜“ä¿¡å·...")
    test_df = test_df.copy()
    test_df['predicted_return'] = mlp.predict(X_test_scaled)
    
    # 8. æ‰§è¡Œ Top-K å›æµ‹ (åŸºäºé¢„æµ‹å€¼)
    capital = INITIAL_CAPITAL
    capital_curve = []
    test_dates = np.sort(test_df['date'].unique())
    
    for date in test_dates:
        daily_data = test_df[test_df['date'] == date]
        
        # é€‰å‡º MLP é¢„æµ‹æ”¶ç›Šæœ€é«˜çš„ K åªè‚¡ç¥¨
        if len(daily_data) < TOP_K:
            selected = daily_data
        else:
            selected = daily_data.nlargest(TOP_K, 'predicted_return')
            
        if not selected.empty:
            # çœŸå®çš„ä¸‹æœŸæ”¶ç›Š
            daily_ret = selected['target'].mean() - 0.0003 # æ‰£è´¹
            capital = capital * (1 + daily_ret)
            
        capital_curve.append({'date': date, 'equity': capital})
        
    # 9. ç»“æœå±•ç¤º
    result_df = pd.DataFrame(capital_curve)
    result_df['date'] = pd.to_datetime(result_df['date'])
    result_df.set_index('date', inplace=True)
    
    total_ret = ((capital / INITIAL_CAPITAL) - 1) * 100
    
    print("=" * 60)
    print(f"ğŸ“Š MLP ç»„åˆç­–ç•¥å›æµ‹ç»“æœ")
    print("-" * 60)
    print(f"è¾“å…¥å› å­æ•°: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†æ—¶æ®µ: {train_df['date'].min().date()} ~ {train_df['date'].max().date()}")
    print(f"å›æµ‹é›†æ—¶æ®µ: {test_df['date'].min().date()} ~ {test_df['date'].max().date()}")
    print(f"æœ€ç»ˆèµ„é‡‘: {capital:,.2f}")
    print(f"åŒºé—´æ”¶ç›Šç‡: {total_ret:.2f}%")
    print("=" * 60)
    
    # ç»˜å›¾
    try:
        plt.figure(figsize=(10, 6))
        plt.plot(result_df.index, result_df['equity'], label='DNN Composite Strategy', color='purple')
        plt.title('Performance of Neural Network Weighted Strategy')
        plt.xlabel('Date')
        plt.ylabel('Equity')
        plt.grid(True)
        plt.legend()
        
        output_img = "./reports/dnn_strategy_curve.png"
        if not os.path.exists("./reports"): os.makedirs("./reports")
        plt.savefig(output_img)
        print(f"èµ„é‡‘æ›²çº¿å·²ä¿å­˜è‡³: {output_img}")
        # plt.show()
    except:
        pass

if __name__ == "__main__":
    train_dnn_strategy()