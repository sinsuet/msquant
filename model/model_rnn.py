import sys
import os
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. ç¯å¢ƒä¸è·¯å¾„è®¾ç½® ---
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½• (modelæ–‡ä»¶å¤¹)
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–çˆ¶ç›®å½• (msquantæ–‡ä»¶å¤¹)
parent_dir = os.path.dirname(current_dir)
# å°†çˆ¶ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(parent_dir)

import pandas as pd
import numpy as np
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler

try:
    from alpha_engine import AlphaContext
except ImportError:
    print("é”™è¯¯: æœªæ‰¾åˆ° alpha_engine.pyã€‚è¯·æ£€æŸ¥è·¯å¾„è®¾ç½®ã€‚")
    sys.exit(1)

# ================= é…ç½® =================
DATA_PATH = os.path.join(parent_dir, "data", "market_data.csv")
REPORT_FILE = os.path.join(parent_dir, "reports", "all_reports.json")
INITIAL_CAPITAL = 1000000
TOP_K = 5
SEQ_LEN = 20
BATCH_SIZE = 128
EPOCHS = 15
LR = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= æ¨¡å‹å®šä¹‰: å¯è§£é‡Š RNN =================
class ExplainableRNN(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2):
        super(ExplainableRNN, self).__init__()
        # 1. ç‰¹å¾æå–å™¨ (RNN)
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)
        
        # 2. æƒé‡ç”Ÿæˆå¤´ (Weight Generator Head)
        # æ ¹æ® RNN æå–çš„å†å²çŠ¶æ€ï¼Œå†³å®šå½“å‰æ¯ä¸ªå› å­çš„æƒé‡
        self.weight_generator = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim), # è¾“å‡ºç»´åº¦ = å› å­æ•°é‡
            nn.Softmax(dim=1)         # ä¿è¯æƒé‡å’Œä¸º 1
        )
        
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        
        # Pass 1: æå–æ—¶åºç‰¹å¾
        out, _ = self.rnn(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€ä½œä¸ºâ€œç¯å¢ƒä¸Šä¸‹æ–‡â€
        context = out[:, -1, :] 
        
        # Pass 2: ç”ŸæˆåŠ¨æ€æƒé‡
        # weights: (batch, input_dim)
        weights = self.weight_generator(context)
        
        # Pass 3: çº¿æ€§ç»„åˆé¢„æµ‹
        # æˆ‘ä»¬ç”¨ç”Ÿæˆçš„æƒé‡ï¼Œå»åŠ æƒâ€œå½“å‰æ—¶åˆ»â€(æœ€åä¸€ä¸ªæ—¶é—´æ­¥)çš„å› å­å€¼
        current_factors = x[:, -1, :]
        prediction = (current_factors * weights).sum(dim=1, keepdim=True)
        
        return prediction, weights

# ================= æ•°æ®å¤„ç† =================
def create_sequences(data, target, seq_len):
    xs, ys = [], []
    for i in range(len(data) - seq_len):
        x = data[i:(i + seq_len)]
        y = target[i + seq_len - 1]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

def run_rnn_strategy():
    print(f"ğŸš€ å¯åŠ¨å¯è§£é‡Š RNN è®­ç»ƒ (Device: {DEVICE})...")
    
    # --- æ•°æ®åŠ è½½ ---
    if not os.path.exists(DATA_PATH):
        print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
        return
    df = pd.read_csv(DATA_PATH)
    df['date'] = pd.to_datetime(df['date'])
    
    if not os.path.exists(REPORT_FILE):
        print(f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {REPORT_FILE}")
        return
    with open(REPORT_FILE, 'r', encoding='utf-8') as f:
        alpha_config = [a for a in json.load(f) if 'error' not in a][:10] # å–å‰10ä¸ªæœ‰æ•ˆå› å­
    
    ctx = AlphaContext(df)
    env = {'CLOSE': ctx.CLOSE(), 'OPEN': ctx.OPEN(), 'VOLUME': ctx.VOLUME(), 'HIGH': ctx.HIGH(), 'LOW': ctx.LOW(), 'DELAY': ctx.DELAY, 'MA': ctx.MA, 'STD': ctx.STD, 'TS_MAX': ctx.TS_MAX, 'TS_MIN': ctx.TS_MIN, 'CORR': ctx.CORR, 'RANK': ctx.RANK}
    
    feature_cols = []
    print("æ­£åœ¨è®¡ç®—å› å­æ•°æ®...", end="")
    for alpha in alpha_config:
        try: 
            col = f"feat_{alpha['name']}"
            df[col] = eval(alpha['formula'], {}, env)
            feature_cols.append(col)
        except: pass
    print("å®Œæˆ")
        
    df['target'] = df.groupby('code')['close'].shift(-1) / df['close'] - 1
    df = df.dropna().sort_values(['code', 'date']).reset_index(drop=True)
    
    split_date = pd.Timestamp("2022-10-26")
    train_raw = df[df['date'] < split_date]
    test_raw = df[df['date'] >= split_date]
    
    scaler = StandardScaler()
    scaler.fit(train_raw[feature_cols])
    
    def process_by_code(sub_df):
        # æ ‡å‡†åŒ–
        sub_df[feature_cols] = scaler.transform(sub_df[feature_cols])
        X = sub_df[feature_cols].values
        y = sub_df['target'].values
        dates = sub_df['date'].values
        
        if len(X) <= SEQ_LEN: return None, None, None
        
        # ç”Ÿæˆåºåˆ—
        X_seq, y_seq = create_sequences(X, y, SEQ_LEN)
        
        # ç”Ÿæˆå¯¹åº”çš„æ—¥æœŸåºåˆ— (ä»ç¬¬ SEQ_LEN-1 ä¸ªå¼€å§‹)
        d_seq = dates[SEQ_LEN-1:]
        
        # === ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶å¯¹é½é•¿åº¦ ===
        # create_sequences çš„ range é€»è¾‘å¯èƒ½å¯¼è‡´æ¯” d_seq å°‘ 1 ä¸ªå…ƒç´ 
        # ä½¿ç”¨ min å–ä¸‰è€…æœ€å°é•¿åº¦ï¼Œç¡®ä¿ä¸€ä¸€å¯¹åº”
        min_len = min(len(X_seq), len(y_seq), len(d_seq))
        
        return X_seq[:min_len], y_seq[:min_len], d_seq[:min_len]

    def build_dataset(raw_df):
        all_X, all_y, all_dates = [], [], []
        for code, sub in raw_df.groupby('code'):
            x, y, d = process_by_code(sub.copy())
            if x is not None: 
                all_X.append(x)
                all_y.append(y)
                all_dates.append(d)
        
        if not all_X:
            return np.array([]), np.array([]), np.array([])
            
        return np.concatenate(all_X), np.concatenate(all_y), np.concatenate(all_dates)

    print("æ­£åœ¨æ„å»ºæ—¶åºæ•°æ® (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    X_train, y_train, _ = build_dataset(train_raw)
    X_test, y_test, dates_test = build_dataset(test_raw)
    
    if len(X_train) == 0:
        print("é”™è¯¯: è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®é‡æˆ– SEQ_LEN è®¾ç½®")
        return

    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)), batch_size=BATCH_SIZE, shuffle=True)
    
    # --- è®­ç»ƒ ---
    model = ExplainableRNN(input_dim=len(feature_cols)).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    criterion = nn.MSELoss()
    
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for X_b, y_b in train_loader:
            X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)
            optimizer.zero_grad()
            preds, _ = model(X_b) # æ¥æ”¶ä¸¤ä¸ªè¾“å‡ºï¼Œåªä¼˜åŒ– pred
            loss = criterion(preds.squeeze(), y_b)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        if (epoch+1)%5==0: 
            print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.6f}")

    # --- å›æµ‹ä¸æƒé‡æå– ---
    print("æ­£åœ¨è¿›è¡Œå›æµ‹...")
    model.eval()
    with torch.no_grad():
        test_tensor = torch.FloatTensor(X_test).to(DEVICE)
        preds, weights = model(test_tensor) # è·å–æƒé‡
        preds = preds.cpu().numpy().flatten()
        weights = weights.cpu().numpy()
        
    # è®¡ç®—æ”¶ç›Š
    backtest_df = pd.DataFrame({'date': dates_test, 'pred': preds, 'target': y_test})
    capital, curve = INITIAL_CAPITAL, []
    unique_dates = np.sort(np.unique(dates_test))
    
    for date in unique_dates:
        daily = backtest_df[backtest_df['date'] == date]
        if len(daily) >= TOP_K:
            ret = daily.nlargest(TOP_K, 'pred')['target'].mean() - 0.0003
            capital *= (1 + ret)
        curve.append(capital)
        
# ... (å‰é¢çš„ä»£ç ä¿æŒä¸å˜) ...
    
    print(f"ğŸ“Š æœ€ç»ˆæ”¶ç›Š: {(capital/INITIAL_CAPITAL-1)*100:.2f}%")
    
    # --- å¯è§†åŒ– ---
    report_dir = os.path.join(parent_dir, 'reports')
    if not os.path.exists(report_dir): os.makedirs(report_dir)

    # 1. èµ„é‡‘æ›²çº¿
    plt.figure(figsize=(10, 4))
    plt.plot(unique_dates, curve, label='BiLSTM Strategy', color='orange')
    plt.title('BiLSTM Equity Curve')
    plt.grid(True)
    plt.legend()
    plt.savefig(os.path.join(report_dir, 'bilstm_equity.png'))
    plt.close() # å…³é—­ç”»å¸ƒé‡Šæ”¾å†…å­˜
    
    # 2. å› å­æƒé‡çƒ­åŠ›å›¾
    # æ„é€  DataFrame
    feature_names = [c.replace('feat_', '') for c in feature_cols]
    w_df = pd.DataFrame(weights, columns=feature_names)
    w_df['date'] = dates_test
    
    # === ã€å…³é”®ä¿®å¤ã€‘ ===
    # æ˜¾å¼åªé€‰å–å› å­åˆ—è¿›è¡Œèšåˆï¼Œæ’é™¤ 'date' åˆ—
    # group key æ˜¯å¹´-æœˆ (ä¾‹å¦‚ "2023-05")
    w_monthly = w_df.groupby(w_df['date'].astype(str).str[:7])[feature_names].mean()
    
    plt.figure(figsize=(12, 6))
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(w_monthly.T, cmap="viridis", annot=False)
    plt.title('BiLSTM Dynamic Factor Weights (Monthly Avg)')
    plt.xlabel('Month')
    plt.ylabel('Factors')
    plt.tight_layout()
    plt.savefig(os.path.join(report_dir, 'bilstm_weights_heatmap.png'))
    plt.close()
    
    print(f"âœ… ç»“æœå·²ä¿å­˜è‡³ {report_dir}")

if __name__ == "__main__":
    run_rnn_strategy()